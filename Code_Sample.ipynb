{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import holidays\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Before training a predictive model, we go through various model preparation steps. After checking for nulls and determining the types of variables present within the data, we discover that many of the variables are binary. Thus, we standardize continuous features and one-hot encode categorical features. More over, we extract featurs such as holiday and day of week to incorporate anything that the date may give us. We do not treat this as a time series task, as we are rich with data and do not see any reason to believe the data has chronological significance.\n",
    "\n",
    "## Model & Feature Selection\n",
    "\n",
    "After evaluating the models (Naive Bayes, Logistic Regression, and XGBoost) on various different metrics and running 5-fold cross validation, XGBoost did the best on this task. Once the model was chosen, we use an embedded method for feature selection. We choose the optimal features for the XGBoost's model prediction. We do this by finding the feature importances for the XGBoost model, and then going through each threshold, lowering the number of features and outputting the number that gives the best precision score. We choose precision, as it seems like the most relevant metric for trying to optimize the ratio of successes.\n",
    "\n",
    "## Autopay\n",
    "\n",
    "Using the precision recall curve and the proposed cost function of a success being +5 and a failure being -1, a function is built to choose the optimal threshold from the precision recall curve to optimize the ratio of success. We optimize particularly on the cost function output, trying to get the cost as high as possible.\n",
    "\n",
    "## Variable X\n",
    "\n",
    "Although I was not able to play with the variable X much, I did notice that after it was put into the pipeline, the number of required features decreased dramatically, making it seem like it captures a lot of information like the continuous variables and the unknown binary variables.\n",
    "\n",
    "## Results\n",
    "\n",
    "I built a run function that will print a report in the jupyter notebook when run. The run function can include variable X. Moreover, make sure the data is in the same directory as the juptyer notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_holidays = holidays.UnitedStates(years = 2017)\n",
    "def holiday(date):\n",
    "    \"\"\"\n",
    "    Purpose is to make a new column that is binary, where 1 means there is a holiday and 0 means there is not.\n",
    "    \"\"\"\n",
    "    if date in us_holidays:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(data, column):\n",
    "    \"\"\"\n",
    "    Takes a pandas data frame and a column name and then linearly interpolates null values.\n",
    "    Puts mean of column for first value, if first value is not present.\n",
    "    \"\"\"\n",
    "    data[column] = data[column].interpolate(method = 'linear').fillna(data[column].mean())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data, without_x = True):\n",
    "    \"\"\"\n",
    "    Function takes in a pandas dataframe with a truth value of whether the x column should be included. What this function\n",
    "    does is:\n",
    "    1) One-hot encode regions.\n",
    "    2) Linearly interpolate null values in model_score, consumer_reporting_score, and weekly_income\n",
    "    3) Standardize non-binary columns, since most of the columns are binary\n",
    "    4) Extract possibly important features from date provided such as day of week and holiday\n",
    "    5) Drop unnecessary columns\n",
    "    \"\"\"\n",
    "    \n",
    "    if without_x:\n",
    "        data = data.drop(['variable_x'], 1)\n",
    "    regions = pd.get_dummies(data.region_code)\n",
    "    data = linear_interpolation(data, 'model_score')\n",
    "    data = linear_interpolation(data, 'consumer_reporting_score')\n",
    "    data = linear_interpolation(data, 'weekly_income')\n",
    "    standardize = StandardScaler()\n",
    "    data[['model_score', 'consumer_reporting_score', 'weekly_income']] = standardize.fit_transform(data[['model_score', 'consumer_reporting_score', 'weekly_income']])\n",
    "    data['date_submitted'] = pd.to_datetime(data['date_submitted'], format=\"%m/%d/%y\")\n",
    "    data['day_of_week'] = data['date_submitted'].dt.day_name()\n",
    "    day_of_week = pd.get_dummies(data.day_of_week)\n",
    "    data = pd.concat([data, day_of_week, regions], 1)\n",
    "    data['holiday'] = data['date_submitted'].map(holiday)\n",
    "    data = data.drop(['date_submitted', 'region_code', 'day_of_week'], 1)\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_model(train_data, train_labels, test_data, test_labels, model):\n",
    "    \"\"\"\n",
    "    Trains and evaluates models and reports out results, including accuracy, train accuracy, confusion matrix, precision,\n",
    "    recall, F1-Score, and then returns a trained model.\n",
    "    \"\"\"\n",
    "    model.fit(train_data, train_labels)\n",
    "    predictions = model.predict(test_data)\n",
    "    score = model.score(test_data, test_labels)\n",
    "    train_score = model.score(train_data, train_labels)\n",
    "    confusion_matrix = cm(test_labels, predictions)\n",
    "    prec = precision(test_labels, predictions)\n",
    "    rec = recall(test_labels, predictions)\n",
    "    f1_score = f1(test_labels, predictions)\n",
    "    \n",
    "    print('Accuracy: {}'.format(score))\n",
    "    print('Train Accuracy: {}'.format(train_score))\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix)\n",
    "    print('Precision: {}'.format(prec))\n",
    "    print('Recall: {}'.format(rec))\n",
    "    print('F1-Score: {}'.format(f1_score))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(model, train_data, train_labels, test_data, test_labels):\n",
    "    \"\"\"\n",
    "    Creates a unique list of thresholds (based on XGBoost), and then selects features on each model, evaluates the model,\n",
    "    and then outputs the best model, train data, and test data.\n",
    "    \"\"\"\n",
    "    thresholds = sorted(list(set(model.feature_importances_)))\n",
    "    best_threshold = []\n",
    "    for thresh in thresholds:\n",
    "        if thresh > 0:\n",
    "            selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "            select_train = selection.transform(train_data)\n",
    "            selection_model = xgb.XGBClassifier()\n",
    "            selection_model.fit(select_train, train_labels)\n",
    "            select_test = selection.transform(test_data)\n",
    "            y_pred = selection_model.predict(select_test)\n",
    "            accuracy = precision(test_labels, y_pred)\n",
    "            print(\"Thresh={}, n={}, Precision:{}\".format(thresh, select_train.shape[1], accuracy*100.0))\n",
    "            best_threshold.append((accuracy, thresh))\n",
    "    \n",
    "    print('\\n Optimal Threshold on Feature Importance')\n",
    "    threshold = sorted(best_threshold, key = lambda tup: tup[0])[-1][1]\n",
    "    selection = SelectFromModel(model, threshold=threshold, prefit=True)\n",
    "    selection_support = selection.get_support()\n",
    "    select_train = selection.transform(train_data)\n",
    "    selection_model = xgb.XGBClassifier()\n",
    "    selection_model.fit(select_train, train_labels)\n",
    "    select_test = selection.transform(test_data)\n",
    "    y_pred = selection_model.predict(select_test)\n",
    "    accuracy = precision(test_labels, y_pred)\n",
    "    print(\"Thresh={}, n={}, Precision:{}\".format(thresh, select_train.shape[1], accuracy*100.0))\n",
    "    print(train_data.columns[selection_support])\n",
    "    select_train = pd.DataFrame(select_train, columns = train_data.columns[selection_support])\n",
    "    select_test = pd.DataFrame(select_test, columns = test_data.columns[selection_support])\n",
    "    \n",
    "    return selection_model, select_train, select_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defining_threshold(model, test_data, test_labels):\n",
    "    \"\"\"\n",
    "    Creates probabilities and precision recall curve for the optimal solution to be solved.\n",
    "    \"\"\"\n",
    "    prediction_probabilities = model.predict_proba(test_data)\n",
    "    precision, recall, thresholds = precision_recall_curve(list(test_labels), prediction_probabilities[:,1])\n",
    "    return optimal_solution(thresholds, prediction_probabilities[:,1], test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_solution(thresholds, test_probabilties, test_labels):\n",
    "    \"\"\"\n",
    "    By going through the thresholds of the precision recall curve, this function does the labeling based on a models\n",
    "    probability of choosing a positive class, and then implements a cost if the function is correct (+5) and if the\n",
    "    function is incorrect (-1). Then it chooses the model with the highest cost, which corresponds to the autopay\n",
    "    system that best optimizes the ratio of success, while making sure failures are minimized.\n",
    "    \"\"\"\n",
    "    optimize = []\n",
    "    for threshold in thresholds:\n",
    "        pred_labels = []\n",
    "        num_success = 0\n",
    "        cost = 0\n",
    "        for probability in test_probabilties:\n",
    "            if probability >= threshold:\n",
    "                pred_labels.append(1)\n",
    "            else:\n",
    "                pred_labels.append(0)\n",
    "        for pred, true in zip(pred_labels, test_labels):\n",
    "            if pred == 1:\n",
    "                if true == pred:\n",
    "                    num_success += 1\n",
    "                    cost += 5\n",
    "                else:\n",
    "                    cost += -1\n",
    "        optimize.append((cost, \n",
    "                         num_success/sum(pred_labels), \n",
    "                         threshold, \n",
    "                         accuracy(test_labels.values, pred_labels), \n",
    "                         precision(test_labels.values, pred_labels), \n",
    "                         recall(test_labels.values, pred_labels), \n",
    "                         f1(test_labels.values, pred_labels)))\n",
    "    \n",
    "    optimal = sorted(optimize, key = lambda tup: tup[0])[-1]\n",
    "    \n",
    "    return 'Cost: {} \\n Ratio of Success: {} \\n Threshold: {} \\n Accuracy: {} \\n Precision: {} \\n Recall: {} \\n F1: {}'.format(optimal[0],\n",
    "                                                                                                                              optimal[1],\n",
    "                                                                                                                              optimal[2],\n",
    "                                                                                                                              optimal[3],\n",
    "                                                                                                                              optimal[4],\n",
    "                                                                                                                              optimal[5],\n",
    "                                                                                                                              optimal[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(without_x = True):\n",
    "    data = pd.read_csv('20200130_v_ 20200123_0318.csv')\n",
    "    processed_data = data_processing(data, without_x)\n",
    "    train_data, test_data = train_test_split(processed_data, \n",
    "                                             test_size = .25, \n",
    "                                             random_state = 28, \n",
    "                                             stratify = processed_data.success)\n",
    "    train_labels = train_data.success\n",
    "    train_data = train_data.drop(['success'], 1)\n",
    "    test_labels = test_data.success\n",
    "    test_data = test_data.drop(['success'], 1)\n",
    "    print('Checking Models \\n')\n",
    "    print('Logistic Regression')\n",
    "    lr = predictive_model(train_data, train_labels, test_data, test_labels, LogisticRegression(solver = 'lbfgs'))\n",
    "    print('\\n XGBoost')\n",
    "    xgboost = predictive_model(train_data, train_labels, test_data, test_labels, xgb.XGBClassifier())\n",
    "    print('\\n Naive Bayes')\n",
    "    nb = predictive_model(train_data, train_labels, test_data, test_labels, GaussianNB())\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Cross Validation \\n')\n",
    "    classifiers = [LogisticRegression(solver = 'lbfgs'), xgb.XGBClassifier(), GaussianNB()]\n",
    "    scores = ['f1', 'precision']\n",
    "    cv_data = processed_data.drop(['success'], 1)\n",
    "    cv_labels = processed_data.success\n",
    "    for classifier in classifiers:\n",
    "        cv = cross_validate(classifier, cv_data, cv_labels, cv = 5, scoring = scores)\n",
    "        print(classifier)\n",
    "        print('F1-Score: {} \\n'.format(cv['test_f1'].mean()))\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Finding Optimal Feature Importance Threshold Using XGBoost')\n",
    "    selected_model, select_train, select_test = feature_selection(xgboost, \n",
    "                                                                      train_data, \n",
    "                                                                      train_labels, \n",
    "                                                                      test_data, \n",
    "                                                                      test_labels)\n",
    "    print('\\n')\n",
    "    print('Final model: XGBoost')\n",
    "    new_xgb = predictive_model(select_train, train_labels, select_test, test_labels, xgb.XGBClassifier())\n",
    "    print('\\n')\n",
    "    print('Accuracy when processing autopay.')\n",
    "    print('\\n')\n",
    "    print(defining_threshold(new_xgb, select_test, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Models \n",
      "\n",
      "Logistic Regression\n",
      "Accuracy: 0.7893602144272229\n",
      "Train Accuracy: 0.7873708336570585\n",
      "Confusion Matrix\n",
      "[[3979 2635]\n",
      " [ 980 9568]]\n",
      "Precision: 0.7840694911087438\n",
      "Recall: 0.90709139173303\n",
      "F1-Score: 0.8411058854555843\n",
      "\n",
      " XGBoost\n",
      "Accuracy: 0.7936137979256497\n",
      "Train Accuracy: 0.7918382410069148\n",
      "Confusion Matrix\n",
      "[[4058 2556]\n",
      " [ 986 9562]]\n",
      "Precision: 0.789074104637729\n",
      "Recall: 0.9065225635191505\n",
      "F1-Score: 0.8437306979617047\n",
      "\n",
      " Naive Bayes\n",
      "Accuracy: 0.7803286330264538\n",
      "Train Accuracy: 0.777348302385207\n",
      "Confusion Matrix\n",
      "[[4147 2467]\n",
      " [1303 9245]]\n",
      "Precision: 0.7893613387978142\n",
      "Recall: 0.8764694728858551\n",
      "F1-Score: 0.8306379155435759\n",
      "\n",
      "\n",
      "Cross Validation \n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "F1-Score: 0.8396007703267487 \n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "F1-Score: 0.8416475613949139 \n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "F1-Score: 0.6045928493179473 \n",
      "\n",
      "\n",
      "\n",
      "Finding Optimal Feature Importance Threshold Using XGBoost\n",
      "Thresh=0.0011268117232248187, n=22, Accuracy:78.90741046377289%\n",
      "Thresh=0.002056947909295559, n=21, Accuracy:78.8991582769434%\n",
      "Thresh=0.002827083459123969, n=20, Accuracy:78.94215694364222%\n",
      "Thresh=0.0034489419776946306, n=19, Accuracy:78.90392868933641%\n",
      "Thresh=0.003577655414119363, n=18, Accuracy:78.91089108910892%\n",
      "Thresh=0.004015563987195492, n=17, Accuracy:78.93042832384253%\n",
      "Thresh=0.0059278784319758415, n=16, Accuracy:78.91610987379362%\n",
      "Thresh=0.006285150535404682, n=15, Accuracy:78.90657211181662%\n",
      "Thresh=0.006756306625902653, n=14, Accuracy:78.77614890462856%\n",
      "Thresh=0.008654412813484669, n=13, Accuracy:78.85864649288709%\n",
      "Thresh=0.013086925260722637, n=12, Accuracy:78.89281895204408%\n",
      "Thresh=0.013462631963193417, n=11, Accuracy:78.86245781545807%\n",
      "Thresh=0.018580418080091476, n=10, Accuracy:78.84694804658461%\n",
      "Thresh=0.019015207886695862, n=9, Accuracy:78.73743200923026%\n",
      "Thresh=0.022154880687594414, n=8, Accuracy:78.71376811594203%\n",
      "Thresh=0.022419104352593422, n=7, Accuracy:78.91932395304305%\n",
      "Thresh=0.02593941055238247, n=6, Accuracy:78.21160743397458%\n",
      "Thresh=0.02617109753191471, n=5, Accuracy:78.23179186037028%\n",
      "Thresh=0.027939388528466225, n=4, Accuracy:78.16970091027308%\n",
      "Thresh=0.03901432454586029, n=3, Accuracy:78.32242225859247%\n",
      "Thresh=0.04474653676152229, n=2, Accuracy:78.50275289670475%\n",
      "Thresh=0.6827933192253113, n=1, Accuracy:78.50275289670475%\n",
      "\n",
      " Optimal Threshold on Feature Importance\n",
      "Thresh=0.6827933192253113, n=20, Accuracy:78.94215694364222%\n",
      "Index(['not_first_attempt', 'purchase_number', 'first_attempt_to_run_card',\n",
      "       'first_or_second_attempt', 'last_payment_status',\n",
      "       'total_prior_unsuccessful_attempts', 'binary_a', 'binary_b',\n",
      "       'model_score', 'consumer_reporting_score', 'weekly_income',\n",
      "       'micro_transaction_present', 'Sunday', 'Thursday', 'Tuesday',\n",
      "       'Wednesday', 'B', 'C', 'D', 'E'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "Final model: XGBoost\n",
      "Accuracy: 0.7941382123295653\n",
      "Train Accuracy: 0.7919547820682153\n",
      "Confusion Matrix\n",
      "[[4062 2552]\n",
      " [ 981 9567]]\n",
      "Precision: 0.7894215694364222\n",
      "Recall: 0.9069965870307167\n",
      "F1-Score: 0.8441346450787489\n",
      "\n",
      "\n",
      "Accuracy when processing autopay.\n",
      "\n",
      "\n",
      "Cost: 47538 \n",
      " Ratio of Success: 0.7240748557759955 \n",
      " Threshold: 0.21818092465400696 \n",
      " Accuracy: 0.7565551800489454 \n",
      " Precision: 0.7240748557759955 \n",
      " Recall: 0.9757299962078119 \n",
      " F1: 0.8312737258702851\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Models \n",
      "\n",
      "Logistic Regression\n",
      "Accuracy: 0.7948374315347861\n",
      "Train Accuracy: 0.7926151814155854\n",
      "Confusion Matrix\n",
      "[[4108 2506]\n",
      " [1015 9533]]\n",
      "Precision: 0.7918431763435502\n",
      "Recall: 0.9037732271520668\n",
      "F1-Score: 0.8441138708106434\n",
      "\n",
      " XGBoost\n",
      "Accuracy: 0.7989162102319077\n",
      "Train Accuracy: 0.7974904824799938\n",
      "Confusion Matrix\n",
      "[[4214 2400]\n",
      " [1051 9497]]\n",
      "Precision: 0.7982684710431202\n",
      "Recall: 0.9003602578687903\n",
      "F1-Score: 0.8462463800400981\n",
      "\n",
      " Naive Bayes\n",
      "Accuracy: 0.7802703647593521\n",
      "Train Accuracy: 0.7777173490793257\n",
      "Confusion Matrix\n",
      "[[4160 2454]\n",
      " [1317 9231]]\n",
      "Precision: 0.789987163029525\n",
      "Recall: 0.8751422070534699\n",
      "F1-Score: 0.8303872621778438\n",
      "\n",
      "\n",
      "Cross Validation \n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "F1-Score: 0.8425153520318185 \n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "F1-Score: 0.8436252509281685 \n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "F1-Score: 0.6241184524317888 \n",
      "\n",
      "\n",
      "\n",
      "Finding Optimal Feature Importance Threshold Using XGBoost\n",
      "Thresh=0.0013128261780366302, n=24, Accuracy:79.82684710431201%\n",
      "Thresh=0.0030505547765642405, n=23, Accuracy:79.8066414459857%\n",
      "Thresh=0.0030638007447123528, n=22, Accuracy:79.75310715485388%\n",
      "Thresh=0.003162466688081622, n=21, Accuracy:79.75820669968937%\n",
      "Thresh=0.0034919274039566517, n=20, Accuracy:79.80672268907563%\n",
      "Thresh=0.004079070873558521, n=19, Accuracy:79.75310715485388%\n",
      "Thresh=0.005107387434691191, n=18, Accuracy:79.81504833963851%\n",
      "Thresh=0.005829283501952887, n=17, Accuracy:79.89071038251366%\n",
      "Thresh=0.006069411523640156, n=16, Accuracy:79.94769697992238%\n",
      "Thresh=0.007679151836782694, n=15, Accuracy:79.72689955600235%\n",
      "Thresh=0.011455098167061806, n=14, Accuracy:79.77641422207279%\n",
      "Thresh=0.013398941606283188, n=13, Accuracy:79.66627536474928%\n",
      "Thresh=0.014413738623261452, n=12, Accuracy:79.66871268813588%\n",
      "Thresh=0.016426632180809975, n=11, Accuracy:79.71626414457018%\n",
      "Thresh=0.016705546528100967, n=10, Accuracy:79.63960929605928%\n",
      "Thresh=0.020824270322918892, n=9, Accuracy:79.87550098064297%\n",
      "Thresh=0.021059392020106316, n=8, Accuracy:80.18754301445286%\n",
      "Thresh=0.02228361740708351, n=7, Accuracy:80.23486745531474%\n",
      "Thresh=0.024666212499141693, n=6, Accuracy:80.15142390088617%\n",
      "Thresh=0.028817830607295036, n=5, Accuracy:80.40558107288327%\n",
      "Thresh=0.029884079471230507, n=4, Accuracy:78.50275289670475%\n",
      "Thresh=0.04403684660792351, n=3, Accuracy:78.50275289670475%\n",
      "Thresh=0.0748038962483406, n=2, Accuracy:78.50275289670475%\n",
      "Thresh=0.6183780431747437, n=1, Accuracy:78.50275289670475%\n",
      "\n",
      " Optimal Threshold on Feature Importance\n",
      "Thresh=0.6183780431747437, n=5, Accuracy:80.40558107288327%\n",
      "Index(['first_attempt_to_run_card', 'last_payment_status',\n",
      "       'total_prior_unsuccessful_attempts', 'variable_x', 'Wednesday'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "Final model: XGBoost\n",
      "Accuracy: 0.7942547488637688\n",
      "Train Accuracy: 0.7905757128428249\n",
      "Confusion Matrix\n",
      "[[4353 2261]\n",
      " [1270 9278]]\n",
      "Precision: 0.8040558107288327\n",
      "Recall: 0.8795980280621919\n",
      "F1-Score: 0.8401322044641644\n",
      "\n",
      "\n",
      "Accuracy when processing autopay.\n",
      "\n",
      "\n",
      "Cost: 47160 \n",
      " Ratio of Success: 0.7039923434509161 \n",
      " Threshold: 0.18980921804904938 \n",
      " Accuracy: 0.7331313366740473 \n",
      " Precision: 0.7039923434509161 \n",
      " Recall: 0.9762988244216914 \n",
      " F1: 0.8180807117890055\n"
     ]
    }
   ],
   "source": [
    "run(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
